---
layout: default
title: CV
---

<section class="cv-header">
    <h1>Fan Yang</h1>
    <p>Machine Learning Researcher and Practitioner</p>
    <div class="contact-info">
        <p>üìç 30 Woorayl St, Carnegie, VIC, 3163, Australia</p>
        <p>üì± +61 415 361 008</p>
        <p>‚úâÔ∏è fan.yang1@monash.edu</p>
    </div>
</section>

<section class="cv-section">
    <h2>Professional Summary</h2>
    <p>Machine Learning researcher and practitioner with PhD in Information Technology and 5+ years of experience developing interpretable AI algorithms. Published in top-tier venues (such as AISTATS, AAMAS, ECML PKDD) with demonstrated expertise in statistical modeling, optimization, and translating complex research into practical applications. Proven track record in both academic research and industry collaboration with strong programming and communication skills.</p>
</section>

<section class="cv-section">
    <h2>Education</h2>
    <div class="cv-item">
        <h3>Doctor of Philosophy (Information Technology PhD Program)</h3>
        <p class="date">01/2021 - 09/2025</p>
        <p>Monash University, Melbourne, Australia</p>
        <p>Department of Data Science and Artificial Intelligence, Faculty of Information Technology</p>
        <p><strong>Thesis:</strong> Boosting and Beyond: Improving the Accuracy-Simplicity Trade-off of Additive Models</p>
        <p><strong>Supervisors:</strong> Dr Mario Boley, Dr Pierre Le Bodic</p>
        <p><strong>Final Thesis submitted:</strong> 12/06/2025</p>
        <h4>Key Contributions during PhD:</h4>
        <ul>
            <li>Perform a theoretical and empirical analyse of the worst-case theoretical gaps of state-of-the-art algorithms for learning interpretable machine learning models</li>
            <li>Propose a novel "Orthogonal Gradient Boosting" algorithm for learning interpretable machine learning models</li>
            <li>Propose a framework which improves both accuracy and transparency of interpretable machine learning models by combining optimisation techniques (Mixed Integer Programming) with interpretable machine learning</li>
            <li>Apply interpretable machine learning models like rule-based models in the field of chemistry to predict the output of Polymerization-Induced Self-Assembly (PISA) reaction</li>
        </ul>
    </div>

    <div class="cv-item">
        <h3>Master of Information Technology</h3>
        <p class="date">02/2017 - 12/2018</p>
        <p>Monash University, Melbourne, Victoria, Australia</p>
    </div>

    <div class="cv-item">
        <h3>Master of Engineering</h3>
        <p class="date">08/2011 - 07/2013</p>
        <p>Harbin Institute of Technology, Harbin, Heilongjiang, China</p>
        <p><strong>Thesis:</strong> Development of a CATIA-based Motion Simulation System for Automatic Fiber Placement Equipment</p>
        <p><strong>Supervisors:</strong> Professor Hua Lu, Professor Zhenyu Han</p>
    </div>

    <div class="cv-item">
        <h3>Bachelor of Engineering</h3>
        <p class="date">08/2007 - 07/2011</p>
        <p>Harbin Institute of Technology, Harbin, Heilongjiang, China</p>
    </div>
</section>

<section class="cv-section">
    <h2>Publications</h2>
    <div class="cv-item">
        <p>Fan Yang, Pierre Le Bodic, and Mario Boley. "Gradient Boosting versus Mixed Integer Programming for Sparse Additive Modeling." Accepted by <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 2025.</p>
    </div>
    <div class="cv-item">
        <p>Yue Yang*, Fan Yang*, Yu Bai, and Hao Wang. "Self-Interpretable Reinforcement Learning via Rule Ensembles." In <em>Proc. of the 24th International Conference on Autonomous Agents and Multiagent Systems</em>, pp. 2235-2243. 2025. <a href="https://dl.acm.org/doi/abs/10.5555/3709347.3743863">Link</a> (* Equal contribution)</p>
    </div>
    <div class="cv-item">
        <p>Fan Yang, Pierre Le Bodic, Michael Kamp, and Mario Boley. "Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles." In <em>International Conference on Artificial Intelligence and Statistics</em>, pp. 1117-1125. PMLR, 2024. <a href="https://proceedings.mlr.press/v238/yang24b.html">Link</a></p>
    </div>
</section>

<section class="cv-section">
    <h2>Technical Skills</h2>
    <div class="cv-item">
        <h3>Programming</h3>
        <p>Python, Java, C++, SQL</p>
    </div>
    <div class="cv-item">
        <h3>ML/AI Frameworks</h3>
        <p>PyTorch, Scikit-learn, TensorFlow</p>
    </div>
    <div class="cv-item">
        <h3>Data Science</h3>
        <p>Statistical modeling, machine learning, data mining, data visualization, big data analytics</p>
    </div>
    <div class="cv-item">
        <h3>Specialized ML</h3>
        <p>Interpretable Machine Learning, ensemble methods, boosting algorithms, reinforcement learning</p>
    </div>
    <div class="cv-item">
        <h3>Optimization</h3>
        <p>Mixed Integer Programming, MiniZinc, discrete optimization</p>
    </div>
</section>

<section class="cv-section">
    <h2>Languages</h2>
    <p>Fluent in Mandarin and English</p>
</section> 